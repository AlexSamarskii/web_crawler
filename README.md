# Go Web Crawler

Многопоточный веб-краулер на языке Go, который безопасно и этично сканирует сайты, извлекает ссылки и хранит данные. Поддерживает `robots.txt`, ограничение частоты запросов, метрики и HTTP API для управления.

---

## Основные возможности

- **Параллельная обработка** — несколько воркеров (горутин) одновременно обрабатывают URL.
- **Очередь URL в Redis** — надёжное хранение и дедупликация.
- **Хранение данных в MongoDB** — структурированное сохранение найденных ссылок.
- **Поддержка `robots.txt`** — уважает запрещённые пути (`Disallow`).
- **Leaky Bucket Rate Limiter** — защита от перегрузки сайтов.

---
```text
+----------------+ +--------+ +-----------+
| HTTP API | --> | Воркеры | --> | Redis |
+----------------+ +--------+ +-----------+
| |
v v
+-----------+ +-----------+
| MongoDB | | Prometheus|
+-----------+ +-----------+
```

- **Redis**: очередь URL, множество посещённых, хранение глубины.
- **MongoDB**: коллекция `urls` с полями `url` (источник) и `link` (найденная ссылка).